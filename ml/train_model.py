# ml/train_model.py
import os
import json
import pandas as pd
import joblib
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# Try import xgboost (optional)
try:
    from xgboost import XGBRegressor
    xgb_available = True
except Exception:
    xgb_available = False

# 1. Load features (generated by create_campaign_features)
FEATURES_CSV = "database/ml_campaign_features.csv"
if not os.path.exists(FEATURES_CSV):
    raise FileNotFoundError(f"{FEATURES_CSV} not found. Run scripts/build_ml_features.py first.")

df = pd.read_csv(FEATURES_CSV)

# 2. Define feature list (pick numeric columns; drop identifiers & target)
drop_cols = {
    "campaign_id", "campaign_name", "platform_id", "objective", "region",
    "avg_roi", "predicted_roi"
}
feature_cols = [c for c in df.columns if c not in drop_cols]

# Some sanity: ensure target exists
if "avg_roi" not in df.columns:
    raise ValueError("Target column 'avg_roi' not found in features CSV")

X = df[feature_cols]
y = df["avg_roi"]

# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. RandomForest (tuned baseline)
rf = RandomForestRegressor(
    n_estimators=300,
    max_depth=12,
    min_samples_split=4,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)
rf_r2 = r2_score(y_test, rf_preds)
rf_mae = mean_absolute_error(y_test, rf_preds)
print(f"RandomForest -> R2: {rf_r2:.4f}, MAE: {rf_mae:.4f}")

best_model = rf
best_score = rf_mae
best_name = "random_forest"

# 5. XGBoost (if available) â€” compare, keep better by MAE
if xgb_available:
    xgb = XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbosity=0
    )
    xgb.fit(X_train, y_train)
    xgb_preds = xgb.predict(X_test)
    xgb_r2 = r2_score(y_test, xgb_preds)
    xgb_mae = mean_absolute_error(y_test, xgb_preds)
    print(f"XGBoost     -> R2: {xgb_r2:.4f}, MAE: {xgb_mae:.4f}")

    if xgb_mae < best_score:
        best_model = xgb
        best_score = xgb_mae
        best_name = "xgboost"

# 6. Save best model and metadata
os.makedirs("ml_models", exist_ok=True)

model_path = os.path.join("ml_models", "rf_tuned.pkl")
joblib.dump(best_model, model_path)

meta_path = os.path.join("ml_models", "rf_tuned_meta.json")
with open(meta_path, "w") as f:
    json.dump({
        "model_name": best_name,
        "model_file": "rf_tuned.pkl",
        "feature_list": feature_cols,
        "feature_count": len(feature_cols),
        "best_mae": float(best_score),
        "rf_metrics": {"mae": rf_mae, "r2": rf_r2}
    }, f, indent=2)

print(f"Saved best model ({best_name}) to {model_path}")
print(f"Saved metadata to {meta_path}")